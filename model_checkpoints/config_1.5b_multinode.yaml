# 1.5B NeuTTS V2 - Multi-Node Config (4 nodes x 8 H100 = 32 GPUs)
# ================================================================
#
# Optimized for 32 GPU distributed training
#
# Usage:
#   SLURM: sbatch slurm_multinode.sh
#   Manual: See launch_multinode.sh

# =============================================================================
# MODEL & TOKENIZER
# =============================================================================
model_path: "pretrained_1.5b_v2"
tokenizer_path: "pretrained_1.5b_v2"

# =============================================================================
# DATA
# =============================================================================
data_path: "/scratch/vikram.solanki/workspace/vs/tts/neutts-air/dataset/encoded/training_v7_encoded.json"
# dataset_name: "username/neutts-v2-encoded"  # Or use HF Hub
max_seq_len: 2048
max_samples: null

# =============================================================================
# OUTPUT
# =============================================================================
save_root: "./outputs"
run_name: "neutts-1.5b-v2-32gpu"

# =============================================================================
# TRAINING - Optimized for 8 GPUs (single node)
# =============================================================================
# Effective batch size = 8 GPUs * 1 per_device * 8 grad_accum = 64
lr: 1e-5
lr_scheduler_type: "cosine"
warmup_ratio: 0.03
weight_decay: 0.01
max_grad_norm: 1.0

per_device_train_batch_size: 1             # Reduced for OOM
gradient_accumulation_steps: 8             # Effective batch = 64

# For 1.8M samples, batch=256: ~7031 steps per epoch
num_epochs: 3
max_steps: -1

# =============================================================================
# CHECKPOINTING
# =============================================================================
save_steps: 500
save_total_limit: 5
resume_from_checkpoint: null

# =============================================================================
# LOGGING
# =============================================================================
logging_steps: 10
report_to: "none"

# =============================================================================
# PERFORMANCE
# =============================================================================
use_flash_attn: true
torch_compile: false
num_workers: 8

# =============================================================================
# MISC
# =============================================================================
seed: 42
