# 1.5B NeuTTS V2 - Multi-Node Config (4 nodes x 8 H100 = 32 GPUs)
# ================================================================
#
# Optimized for 32 GPU distributed training
#
# Usage:
#   SLURM: sbatch slurm_multinode.sh
#   Manual: See launch_multinode.sh

# =============================================================================
# MODEL & TOKENIZER
# =============================================================================
model_path: "pretrained_1.5b_v2"
tokenizer_path: "pretrained_1.5b_v2"

# =============================================================================
# DATA
# =============================================================================
data_path: "./encoded_v2_1.5b.json"
# dataset_name: "username/neutts-v2-encoded"  # Or use HF Hub
max_seq_len: 8192
max_samples: null

# =============================================================================
# OUTPUT
# =============================================================================
save_root: "./outputs"
run_name: "neutts-1.5b-v2-32gpu"

# =============================================================================
# TRAINING - Optimized for 32 GPUs
# =============================================================================
# Effective batch size = 32 GPUs * 4 per_device * 2 grad_accum = 256
lr: 1e-5                                   # Lower LR for large batch
lr_scheduler_type: "cosine"
warmup_ratio: 0.03
weight_decay: 0.01
max_grad_norm: 1.0

per_device_train_batch_size: 4             # H100 80GB can handle more
gradient_accumulation_steps: 2             # Effective batch = 256

# For 1.8M samples, batch=256: ~7031 steps per epoch
num_epochs: 3
max_steps: -1

# =============================================================================
# CHECKPOINTING
# =============================================================================
save_steps: 500
save_total_limit: 5
resume_from_checkpoint: null

# =============================================================================
# LOGGING
# =============================================================================
logging_steps: 10
report_to: "none"

# =============================================================================
# PERFORMANCE
# =============================================================================
use_flash_attn: true
torch_compile: false
num_workers: 8

# =============================================================================
# MISC
# =============================================================================
seed: 42
